{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsnCPbdkxYZd"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <h1 style=\"color: #FF6347;\">Self-Guided Lab: Retrieval-Augmented Generation (RAGs)</h1>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZp4BQAVxYZj"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExZ3FsdzRveTBrenMxM3VnbDMwaTJxN2NnZm50aGFibXk1NzNnY2Q0MCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/LR5ZBwZHv02lmpVoEU/giphy.gif\" alt=\"NLP Gif\" style=\"width: 300px; height: 150px; object-fit: cover; object-position: center;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gizk6HCYxYZo"
      },
      "source": [
        "<h1 style=\"color: #FF6347;\">Data Storage & Retrieval</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW5UOI8ZxYZp"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">PyPDFLoader</h2>\n",
        "\n",
        "`PyPDFLoader` is a lightweight Python library designed to streamline the process of loading and parsing PDF documents for text processing tasks. It is particularly useful in Retrieval-Augmented Generation workflows where text extraction from PDFs is required.\n",
        "\n",
        "- **What Does PyPDFLoader Do?**\n",
        "  - Extracts text from PDF files, retaining formatting and layout.\n",
        "  - Simplifies the preprocessing of document-based datasets.\n",
        "  - Supports efficient and scalable loading of large PDF collections.\n",
        "\n",
        "- **Key Features:**\n",
        "  - Compatible with popular NLP libraries and frameworks.\n",
        "  - Handles multi-page PDFs and embedded images (e.g., OCR-compatible setups).\n",
        "  - Provides flexible configurations for structured text extraction.\n",
        "\n",
        "- **Use Cases:**\n",
        "  - Preparing PDF documents for retrieval-based systems in RAGs.\n",
        "  - Automating the text extraction pipeline for document analysis.\n",
        "  - Creating datasets from academic papers, technical manuals, and reports.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (1.0.8)\n",
            "Requirement already satisfied: langchain_community in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.4.1)\n",
            "Requirement already satisfied: pypdf in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (6.3.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.6 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain) (1.0.7)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain) (1.0.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain) (2.12.4)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (0.4.45)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /Users/Shyam/Library/Python/3.13/lib/python/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (4.15.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.6->langchain) (3.0.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (0.25.0)\n",
            "Requirement already satisfied: anyio in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.11.0)\n",
            "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
            "Requirement already satisfied: idna in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain_community) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain_community) (2.0.44)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain_community) (3.13.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain_community) (2.12.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain_community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=2.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain_community) (2.3.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.3.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install langchain langchain_community pypdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting termcolor\n",
            "  Using cached termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: langchain_openai in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (1.0.3)\n",
            "Collecting langchain-huggingface\n",
            "  Using cached langchain_huggingface-1.0.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting sentence-transformers\n",
            "  Using cached sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: chromadb in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (1.3.5)\n",
            "Collecting langchain_chroma\n",
            "  Using cached langchain_chroma-1.0.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: tiktoken in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.12.0)\n",
            "Requirement already satisfied: openai in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.8.1)\n",
            "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (1.2.1)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain_openai) (1.0.7)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from tiktoken) (2.32.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (2.12.4)\n",
            "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.2->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.2->langchain_openai) (0.4.45)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /Users/Shyam/Library/Python/3.13/lib/python/site-packages (from langchain-core<2.0.0,>=1.0.2->langchain_openai) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.2->langchain_openai) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.2->langchain_openai) (9.1.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.2->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain_openai) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain_openai) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Collecting huggingface-hub<1.0.0,>=0.33.4 (from langchain-huggingface)\n",
            "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain-huggingface) (0.22.1)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.10.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (1.2.0)\n",
            "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
            "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
            "INFO: pip is looking at multiple versions of sentence-transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting sentence-transformers\n",
            "  Using cached sentence_transformers-5.1.1-py3-none-any.whl.metadata (16 kB)\n",
            "  Using cached sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
            "  Using cached sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "  Using cached sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "  Using cached sentence_transformers-4.0.2-py3-none-any.whl.metadata (13 kB)\n",
            "  Using cached sentence_transformers-4.0.1-py3-none-any.whl.metadata (13 kB)\n",
            "  Using cached sentence_transformers-4.0.0-py3-none-any.whl.metadata (13 kB)\n",
            "INFO: pip is still looking at multiple versions of sentence-transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
            "  Using cached sentence_transformers-3.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "  Using cached sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
            "  Using cached sentence_transformers-3.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "  Using cached sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\n",
            "  Using cached sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
            "  Using cached sentence_transformers-3.1.0-py3-none-any.whl.metadata (23 kB)\n",
            "  Using cached sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "  Using cached sentence_transformers-3.0.0-py3-none-any.whl.metadata (10 kB)\n",
            "  Using cached sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\n",
            "  Using cached sentence_transformers-2.6.1-py3-none-any.whl.metadata (11 kB)\n",
            "  Using cached sentence_transformers-2.6.0-py3-none-any.whl.metadata (11 kB)\n",
            "  Using cached sentence_transformers-2.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "  Using cached sentence_transformers-2.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "  Using cached sentence_transformers-2.4.0-py3-none-any.whl.metadata (11 kB)\n",
            "  Using cached sentence_transformers-2.3.1-py3-none-any.whl.metadata (11 kB)\n",
            "  Using cached sentence_transformers-2.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "  Using cached sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-2.2.1.tar.gz (84 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-2.2.0.tar.gz (79 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-2.1.0.tar.gz (78 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-2.0.0.tar.gz (85 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-1.2.1.tar.gz (80 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-1.2.0.tar.gz (81 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-1.1.1.tar.gz (81 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-1.1.0.tar.gz (78 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-1.0.4.tar.gz (74 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-1.0.3.tar.gz (74 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-1.0.2.tar.gz (74 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-1.0.1.tar.gz (74 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-1.0.0.tar.gz (74 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.4.1.2.tar.gz (64 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.4.1.1.tar.gz (64 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.4.1.tar.gz (64 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.4.0.tar.gz (65 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.3.9.tar.gz (64 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting transformers<3.6.0,>=3.1.0 (from sentence-transformers)\n",
            "  Using cached transformers-3.5.1-py3-none-any.whl.metadata (32 kB)\n",
            "Collecting sentence-transformers\n",
            "  Using cached sentence-transformers-0.3.8.tar.gz (66 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting transformers<3.4.0,>=3.1.0 (from sentence-transformers)\n",
            "  Using cached transformers-3.3.1-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting sentence-transformers\n",
            "  Using cached sentence-transformers-0.3.7.2.tar.gz (59 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.3.7.1.tar.gz (59 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.3.7.tar.gz (59 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.3.6.tar.gz (62 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting transformers<3.2.0,>=3.1.0 (from sentence-transformers)\n",
            "  Using cached transformers-3.1.0-py3-none-any.whl.metadata (49 kB)\n",
            "Collecting sentence-transformers\n",
            "  Using cached sentence-transformers-0.3.5.1.tar.gz (61 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting transformers==3.0.2 (from sentence-transformers)\n",
            "  Using cached transformers-3.0.2-py3-none-any.whl.metadata (44 kB)\n",
            "Collecting sentence-transformers\n",
            "  Using cached sentence-transformers-0.3.5.tar.gz (61 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.3.4.tar.gz (61 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.3.3.tar.gz (65 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.3.2.tar.gz (65 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.3.1.tar.gz (64 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.3.0.tar.gz (61 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.2.6.2.tar.gz (60 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting transformers==2.11.0 (from sentence-transformers)\n",
            "  Using cached transformers-2.11.0-py3-none-any.whl.metadata (45 kB)\n",
            "Collecting sentence-transformers\n",
            "  Using cached sentence-transformers-0.2.6.1.tar.gz (55 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.2.5.1.tar.gz (52 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting transformers==2.3.0 (from sentence-transformers)\n",
            "  Using cached transformers-2.3.0-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting sentence-transformers\n",
            "  Using cached sentence-transformers-0.2.5.tar.gz (49 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.2.4.1.tar.gz (49 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting transformers==2.2.1 (from sentence-transformers)\n",
            "  Using cached transformers-2.2.1-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting sentence-transformers\n",
            "  Using cached sentence-transformers-0.2.4.tar.gz (49 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.2.3.tar.gz (45 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting pytorch-transformers==1.1.0 (from sentence-transformers)\n",
            "  Using cached pytorch_transformers-1.1.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting sentence-transformers\n",
            "  Using cached sentence-transformers-0.2.2.tar.gz (44 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.2.1.tar.gz (42 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting pytorch-transformers==1.0.0 (from sentence-transformers)\n",
            "  Using cached pytorch_transformers-1.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting sentence-transformers\n",
            "  Using cached sentence-transformers-0.2.0.tar.gz (28 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.1.0.tar.gz (35 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting termcolor\n",
            "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting tokenizers<1.0.0,>=0.19.1 (from langchain-huggingface)\n",
            "  Using cached tokenizers-0.22.1-cp39-abi3-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting huggingface-hub<1.0.0,>=0.33.4 (from langchain-huggingface)\n",
            "  Using cached huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting langchain-huggingface\n",
            "  Using cached langchain_huggingface-1.0.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting typing-extensions<5,>=4.11 (from openai)\n",
            "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<2.0.0,>=1.0.2->langchain_openai)\n",
            "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting pyyaml<7.0.0,>=5.3.0 (from langchain-core<2.0.0,>=1.0.2->langchain_openai)\n",
            "  Using cached pyyaml-6.0.3-cp313-cp313-macosx_10_13_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting pydantic-core==2.41.5 (from pydantic<3,>=1.9.0->openai)\n",
            "  Using cached pydantic_core-2.41.5-cp313-cp313-macosx_10_12_x86_64.whl.metadata (7.3 kB)\n",
            "Collecting pydantic<3,>=1.9.0 (from openai)\n",
            "  Using cached pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
            "Collecting packaging<26.0.0,>=23.2.0 (from langchain-core<2.0.0,>=1.0.2->langchain_openai)\n",
            "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core<2.0.0,>=1.0.2->langchain_openai)\n",
            "  Using cached langsmith-0.4.45-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=1.0.2->langchain_openai)\n",
            "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-core<2.0.0,>=1.0.2 (from langchain_openai)\n",
            "  Using cached langchain_core-1.0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting jiter<1,>=0.10.0 (from openai)\n",
            "  Using cached jiter-0.12.0-cp313-cp313-macosx_10_12_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting distro<2,>=1.7.0 (from openai)\n",
            "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting anyio<5,>=3.5.0 (from openai)\n",
            "  Using cached anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting openai\n",
            "  Using cached openai-2.8.1-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting tiktoken\n",
            "  Using cached tiktoken-0.12.0-cp313-cp313-macosx_10_13_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting langchain_openai\n",
            "  Using cached langchain_openai-1.0.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "\u001b[31mERROR: Cannot install sentence-transformers==0.1.0, sentence-transformers==0.2.0, sentence-transformers==0.2.1, sentence-transformers==0.2.2, sentence-transformers==0.2.3, sentence-transformers==0.2.4, sentence-transformers==0.2.4.1, sentence-transformers==0.2.5, sentence-transformers==0.2.5.1, sentence-transformers==0.2.6.1, sentence-transformers==0.2.6.2, sentence-transformers==0.3.0, sentence-transformers==0.3.1, sentence-transformers==0.3.2, sentence-transformers==0.3.3, sentence-transformers==0.3.4, sentence-transformers==0.3.5, sentence-transformers==0.3.5.1, sentence-transformers==0.3.6, sentence-transformers==0.3.7, sentence-transformers==0.3.7.1, sentence-transformers==0.3.7.2, sentence-transformers==0.3.8, sentence-transformers==0.3.9, sentence-transformers==0.4.0, sentence-transformers==0.4.1, sentence-transformers==0.4.1.1, sentence-transformers==0.4.1.2, sentence-transformers==1.0.0, sentence-transformers==1.0.1, sentence-transformers==1.0.2, sentence-transformers==1.0.3, sentence-transformers==1.0.4, sentence-transformers==1.1.0, sentence-transformers==1.1.1, sentence-transformers==1.2.0, sentence-transformers==1.2.1, sentence-transformers==2.0.0, sentence-transformers==2.1.0, sentence-transformers==2.2.0, sentence-transformers==2.2.1, sentence-transformers==2.2.2, sentence-transformers==2.3.0, sentence-transformers==2.3.1, sentence-transformers==2.4.0, sentence-transformers==2.5.0, sentence-transformers==2.5.1, sentence-transformers==2.6.0, sentence-transformers==2.6.1, sentence-transformers==2.7.0, sentence-transformers==3.0.0, sentence-transformers==3.0.1, sentence-transformers==3.1.0, sentence-transformers==3.1.1, sentence-transformers==3.2.0, sentence-transformers==3.2.1, sentence-transformers==3.3.0, sentence-transformers==3.3.1, sentence-transformers==3.4.0, sentence-transformers==3.4.1, sentence-transformers==4.0.0, sentence-transformers==4.0.1, sentence-transformers==4.0.2, sentence-transformers==4.1.0, sentence-transformers==5.0.0, sentence-transformers==5.1.0, sentence-transformers==5.1.1 and sentence-transformers==5.1.2 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    sentence-transformers 5.1.2 depends on torch>=1.11.0\n",
            "    sentence-transformers 5.1.1 depends on torch>=1.11.0\n",
            "    sentence-transformers 5.1.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 5.0.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 4.1.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 4.0.2 depends on torch>=1.11.0\n",
            "    sentence-transformers 4.0.1 depends on torch>=1.11.0\n",
            "    sentence-transformers 4.0.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 3.4.1 depends on torch>=1.11.0\n",
            "    sentence-transformers 3.4.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 3.3.1 depends on torch>=1.11.0\n",
            "    sentence-transformers 3.3.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 3.2.1 depends on torch>=1.11.0\n",
            "    sentence-transformers 3.2.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 3.1.1 depends on torch>=1.11.0\n",
            "    sentence-transformers 3.1.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 3.0.1 depends on torch>=1.11.0\n",
            "    sentence-transformers 3.0.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 2.7.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 2.6.1 depends on torch>=1.11.0\n",
            "    sentence-transformers 2.6.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 2.5.1 depends on torch>=1.11.0\n",
            "    sentence-transformers 2.5.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 2.4.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 2.3.1 depends on torch>=1.11.0\n",
            "    sentence-transformers 2.3.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 2.2.2 depends on torch>=1.6.0\n",
            "    sentence-transformers 2.2.1 depends on torch>=1.6.0\n",
            "    sentence-transformers 2.2.0 depends on torch>=1.6.0\n",
            "    sentence-transformers 2.1.0 depends on torch>=1.6.0\n",
            "    sentence-transformers 2.0.0 depends on torch>=1.6.0\n",
            "    sentence-transformers 1.2.1 depends on torch>=1.6.0\n",
            "    sentence-transformers 1.2.0 depends on torch>=1.6.0\n",
            "    sentence-transformers 1.1.1 depends on torch>=1.6.0\n",
            "    sentence-transformers 1.1.0 depends on torch>=1.6.0\n",
            "    sentence-transformers 1.0.4 depends on torch>=1.6.0\n",
            "    sentence-transformers 1.0.3 depends on torch>=1.6.0\n",
            "    sentence-transformers 1.0.2 depends on torch>=1.6.0\n",
            "    sentence-transformers 1.0.1 depends on torch>=1.6.0\n",
            "    sentence-transformers 1.0.0 depends on torch>=1.6.0\n",
            "    sentence-transformers 0.4.1.2 depends on torch>=1.6.0\n",
            "    sentence-transformers 0.4.1.1 depends on torch>=1.6.0\n",
            "    sentence-transformers 0.4.1 depends on torch>=1.6.0\n",
            "    sentence-transformers 0.4.0 depends on torch>=1.6.0\n",
            "    sentence-transformers 0.3.9 depends on torch>=1.6.0\n",
            "    sentence-transformers 0.3.8 depends on torch>=1.2.0\n",
            "    sentence-transformers 0.3.7.2 depends on torch>=1.2.0\n",
            "    sentence-transformers 0.3.7.1 depends on torch>=1.2.0\n",
            "    sentence-transformers 0.3.7 depends on torch>=1.2.0\n",
            "    sentence-transformers 0.3.6 depends on torch>=1.2.0\n",
            "    sentence-transformers 0.3.5.1 depends on torch>=1.2.0\n",
            "    sentence-transformers 0.3.5 depends on torch>=1.2.0\n",
            "    sentence-transformers 0.3.4 depends on torch>=1.2.0\n",
            "    sentence-transformers 0.3.3 depends on torch>=1.2.0\n",
            "    sentence-transformers 0.3.2 depends on torch>=1.2.0\n",
            "    sentence-transformers 0.3.1 depends on torch>=1.2.0\n",
            "    sentence-transformers 0.3.0 depends on torch>=1.0.1\n",
            "    sentence-transformers 0.2.6.2 depends on torch>=1.0.1\n",
            "    sentence-transformers 0.2.6.1 depends on torch>=1.0.1\n",
            "    sentence-transformers 0.2.5.1 depends on torch>=1.0.1\n",
            "    sentence-transformers 0.2.5 depends on torch>=1.0.1\n",
            "    sentence-transformers 0.2.4.1 depends on torch>=1.0.1\n",
            "    sentence-transformers 0.2.4 depends on torch>=1.0.1\n",
            "    sentence-transformers 0.2.3 depends on torch>=1.0.1\n",
            "    sentence-transformers 0.2.2 depends on torch>=1.0.1\n",
            "    sentence-transformers 0.2.1 depends on torch>=1.0.1\n",
            "    sentence-transformers 0.2.0 depends on torch>=1.0.1\n",
            "    sentence-transformers 0.1.0 depends on torch>=1.0.1\n",
            "\n",
            "Additionally, some packages in these conflicts have no matching distributions available for your environment:\n",
            "    torch\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install termcolor langchain_openai langchain-huggingface sentence-transformers chromadb langchain_chroma tiktoken openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6heKZkQUxYZr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRS44B2XxYZs",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "<h3 style=\"color: #FF8C00;\">Loading the Documents</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cuREtJRixYZt"
      },
      "outputs": [],
      "source": [
        "# File path for the document\n",
        "\n",
        "file_path = \"/Users/Shyam/Desktop/Ironhack-Bootcamp/Week 7/D5/lab-intro-rag/ai-for-everyone.pdf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz_8SOLxxYZt"
      },
      "source": [
        "<h3 style=\"color: #FF8C00;\">Documents into pages</h3>\n",
        "\n",
        "The `PyPDFLoader` library allows efficient loading and splitting of PDF documents into smaller, manageable parts for NLP tasks.\n",
        "\n",
        "This functionality is particularly useful in workflows requiring granular text processing, such as Retrieval-Augmented Generation (RAG).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_b5Z_45UxYZu",
        "outputId": "a600d69f-14fe-4492-f236-97261d6ff36c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "297"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load and split the document\n",
        "loader = PyPDFLoader(file_path)\n",
        "pages = loader.load_and_split()\n",
        "len(pages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt50NRQaxYZv"
      },
      "source": [
        "<h3 style=\"color: #FF8C00;\">Pages into Chunks</h3>\n",
        "\n",
        "\n",
        "####  RecursiveCharacterTextSplitter in LangChain\n",
        "\n",
        "The `RecursiveCharacterTextSplitter` is the **recommended splitter** in LangChain when you want to break down long documents into smaller, semantically meaningful chunks  especially useful in **RAG pipelines**, where clean context chunks lead to better LLM responses.\n",
        "\n",
        "####  Parameters\n",
        "\n",
        "| Parameter       | Description                                                                 |\n",
        "|-----------------|-----------------------------------------------------------------------------|\n",
        "| `chunk_size`    | The **maximum number of characters** allowed in a chunk (e.g., `1000`).     |\n",
        "| `chunk_overlap` | The number of **overlapping characters** between consecutive chunks (e.g., `200`). This helps preserve context continuity. |\n",
        "\n",
        "####  How it works\n",
        "`RecursiveCharacterTextSplitter` attempts to split the text **intelligently**, trying the following separators in order:\n",
        "1. Paragraphs (`\"\\n\\n\"`)\n",
        "2. Lines (`\"\\n\"`)\n",
        "3. Sentences or words (`\" \"`)\n",
        "4. Individual characters (as a last resort)\n",
        "\n",
        "This makes it ideal for handling **natural language documents**, such as PDFs, articles, or long reports, without breaking sentences or paragraphs in awkward ways.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1096"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "chunks = text_splitter.split_documents(pages)\n",
        "\n",
        "len(chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####  Alternative: CharacterTextSplitter\n",
        "\n",
        "`CharacterTextSplitter` is a simpler splitter that breaks text into chunks based **purely on character count**, without trying to preserve any natural language structure.\n",
        "\n",
        "##### Example:\n",
        "```python\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "````\n",
        "\n",
        "This method is faster and more predictable but may split text in the middle of a sentence or paragraph, which can hurt performance in downstream tasks like retrieval or QA.\n",
        "\n",
        "---\n",
        "\n",
        "#### Comparison Table\n",
        "\n",
        "| Feature                        | RecursiveCharacterTextSplitter | CharacterTextSplitter     |\n",
        "| ------------------------------ | ------------------------------ | ------------------------- |\n",
        "| Structure-aware splitting      |  Yes                          |  No                      |\n",
        "| Preserves sentence/paragraphs  |  Yes                          |  No                      |\n",
        "| Risk of splitting mid-sentence |  Minimal                     |  High                   |\n",
        "| Ideal for RAG/document QA      |  Highly recommended           |  Only if structured text |\n",
        "| Performance speed              |  Slightly slower             |  Faster                  |\n",
        "\n",
        "---\n",
        "\n",
        "#### Recommendation\n",
        "\n",
        "Use `RecursiveCharacterTextSplitter` for most real-world document processing tasks, especially when building RAG pipelines or working with structured natural language content like PDFs or articles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices for Choosing Chunk Size in RAG\n",
        "\n",
        "### Best Practices for Chunk Size in RAG\n",
        "\n",
        "| Factor                      | Recommendation                                                                                                                                                                                          |\n",
        "| ---------------------------| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **LLM context limit**       | Choose a chunk size that lets you retrieve multiple chunks **without exceeding the models token limit**. For example, GPT-4o supports 128k tokens, but with GPT-3.5 (16k) or GPT-4 (32k), keep it modest. |\n",
        "| **Chunk size (in characters)** | Typically: **5001,000 characters** per chunk  ~75200 tokens. This fits well for retrieval + prompt without context overflow.                                                                           |\n",
        "| **Chunk size (in tokens)**  | If using token-based splitter (e.g. `TokenTextSplitter`): aim for **100300 tokens** per chunk.                                                                                                            |\n",
        "| **Chunk overlap**           | Use **overlap of 1030%** (e.g., 100300 characters or ~50 tokens) to preserve context across chunk boundaries and avoid cutting off important ideas mid-sentence.                                        |\n",
        "| **Document structure**      | Use **`RecursiveCharacterTextSplitter`** to preserve semantic boundaries (paragraphs, sentences) instead of arbitrary cuts.                                                                                |\n",
        "| **Task type**               | For **question answering**, smaller chunks (~500800 chars) reduce noise.<br>For **summarization**, slightly larger chunks (~10001500) are OK.                                                          |\n",
        "| **Embedding model**         | Some models (e.g., `text-embedding-3-large`) can handle long input. But still, smaller chunks give **finer-grained retrieval**, which improves relevance.                                                  |\n",
        "| **Query type**              | If users ask **very specific questions**, small focused chunks are better. For broader queries, bigger chunks might help.                                                                                  |\n",
        "\n",
        "\n",
        "### Rule of Thumb\n",
        "\n",
        "| Use Case                 | Chunk Size      | Overlap |\n",
        "| ------------------------| --------------- | ------- |\n",
        "| Factual Q&A              | 500800 chars   | 100200 |\n",
        "| Summarization            | 10001500 chars | 200300 |\n",
        "| Technical documents      | 400700 chars   | 100200 |\n",
        "| Long reports/books       | 8001200 chars  | 200300 |\n",
        "| Small LLMs (16k tokens) | 800 chars      | 100200 |\n",
        "\n",
        "\n",
        "### Avoid\n",
        "\n",
        "- Chunks >2000 characters: risks context overflow.\n",
        "- No overlap: may lose key information between chunks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg15RjVPxYZw"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">Embeddings</h2>\n",
        "\n",
        "Embeddings transform text into dense vector representations, capturing semantic meaning and contextual relationships. They are essential for efficient document retrieval and similarity analysis.\n",
        "\n",
        "- **What are OpenAI Embeddings?**\n",
        "  - Pre-trained embeddings like `text-embedding-3-large` generate high-quality vector representations for text.\n",
        "  - Encapsulate semantic relationships in the text, enabling robust NLP applications.\n",
        "\n",
        "- **Key Features of `text-embedding-3-large`:**\n",
        "  - Large-scale embedding model optimized for accuracy and versatility.\n",
        "  - Handles diverse NLP tasks, including retrieval, classification, and clustering.\n",
        "  - Ideal for applications with high-performance requirements.\n",
        "\n",
        "- **Benefits:**\n",
        "  - Reduces the need for extensive custom training.\n",
        "  - Provides state-of-the-art performance in retrieval-augmented systems.\n",
        "  - Compatible with RAGs to create powerful context-aware models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "L0xDxElwxYZw"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_WRIo3_0xYZx",
        "outputId": "78bfbbf3-9d25-4e31-bdbc-3e932e6bbfec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "MNZfTng5xYZz",
        "outputId": "db1a7c85-ef9f-447e-92cd-9d097e959847"
      },
      "outputs": [],
      "source": [
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsSA7RKvxYZz"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">ChromaDB</h2>\n",
        "\n",
        "ChromaDB is a versatile vector database designed for efficiently storing and retrieving embeddings. It integrates seamlessly with embedding models to enable high-performance similarity search and context-based retrieval.\n",
        "\n",
        "### Workflow Overview:\n",
        "- **Step 1:** Generate embeddings using a pre-trained model (e.g., OpenAI's `text-embedding-3-large`).\n",
        "- **Step 2:** Store the embeddings in ChromaDB for efficient retrieval and similarity calculations.\n",
        "- **Step 3:** Use the stored embeddings to perform searches, matching, or context-based retrieval.\n",
        "\n",
        "### Key Features of ChromaDB:\n",
        "- **Scalability:** Handles large-scale datasets with optimized indexing and search capabilities.\n",
        "- **Speed:** Provides fast and accurate retrieval of embeddings for real-time applications.\n",
        "- **Integration:** Supports integration with popular frameworks and libraries for embedding generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "brKe6wUgxYZ0"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "VkjHR-RkxYZ0",
        "outputId": "bc11bda9-f283-457a-f584-5a06b95c4dd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChromaDB created with document embeddings.\n"
          ]
        }
      ],
      "source": [
        "db = Chroma.from_documents(chunks, embeddings, persist_directory=\"./chroma_db_LAB\")\n",
        "print(\"ChromaDB created with document embeddings.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27OdN1IVxYZ1"
      },
      "source": [
        "<h1 style=\"color: #FF6347;\">Retrieving Documents</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice1: Write a user question that someone might ask about your books topic or content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XiLv-TfrxYZ1"
      },
      "outputs": [],
      "source": [
        "user_question = \"\" # User question\n",
        "retrieved_docs = db.similarity_search(user_question, k=10) # k is the number of documents to retrieve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qgWsh50JxYZ1",
        "outputId": "c8640c5d-5955-471f-fdd2-37096f5f68c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document 1:\n",
            "f Human Communication. Palo Alto, CA: \n",
            "Science and Behavior Books.\n",
            "Weizenbaum, J. 1976. Computer Power and Human Reason: From Judgment to \n",
            "Calculation. San Francisco: W . H. Freeman.\n",
            "Document 2:\n",
            " and when the difference between human \n",
            "and machine is affirmed at the cost of their unity that is negated  done so by \n",
            "disconnections. The way out is the establishment of a relation through affirm -\n",
            "ing both the identity of, and the difference between, the two sides  as done by\n",
            "Document 3:\n",
            "ne, Not a Camera: How Financial Models Shape \n",
            "Markets. (1st edn.). Cambridge, MA: The MIT Press.\n",
            "Malik, M. M. 2020. A Hierarchy of Limitations in Machine Learning. \n",
            "ArXiv:2002.05193 [Cs, Econ, Math, Stat] , February. http://arxiv.org \n",
            "/abs/2002.05193.\n",
            "Marcus, G. 2018. Deep Learning: A Critical Appraisal. ArXiv:1801.00631 [Cs, \n",
            "Stat], January. http://arxiv.org/abs/1801.00631.\n",
            "McQuillan, D. 2015. Algorithmic States of Exception. European Journal  \n",
            "of Cultural Studies  18 (45), 564576. DOI: https://doi.org/10.1177 \n",
            "/1367549415577389.\n",
            "McQuillan, D. 2017. Data Science as Machinic Neoplatonism. Philosophy & \n",
            "Technolog y, August, 120. DOI: https://doi.org/10.1007/s13347-017-0273-3.\n",
            "McQuillan, D. 2018. Peoples Councils for Ethical Machine Learning. Social \n",
            "Media + Society 4 (2). DOI: https://doi.org/10.1177/2056305118768303.\n",
            "Mitchell, A. 2015. Posthumanist Post-Colonialism? Worldly (blog). 26 Feb -\n"
          ]
        }
      ],
      "source": [
        "# Display top results\n",
        "for i, doc in enumerate(retrieved_docs[:3]): # Display top 3 results\n",
        "    print(f\"Document {i+1}:\\n{doc.page_content[36:1000]}\") # Display content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuGK8gL6xYZ1"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">Preparing Content for GenAI</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2iB3lZqHxYZ2"
      },
      "outputs": [],
      "source": [
        "def _get_document_prompt(docs):\n",
        "    prompt = \"\\n\"\n",
        "    for doc in docs:\n",
        "        prompt += \"\\nContent:\\n\"\n",
        "        prompt += doc.page_content + \"\\n\\n\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2okzmuADxYZ2",
        "outputId": "0aa6cdca-188d-40e0-f5b4-8888d3549ea4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context formatted for GPT model.\n"
          ]
        }
      ],
      "source": [
        "# Generate a formatted context from the retrieved documents\n",
        "formatted_context = _get_document_prompt(retrieved_docs)\n",
        "print(\"Context formatted for GPT model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzIczQNTxYZ2"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">ChatBot Architecture</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice2: Write a prompt that is relevant and tailored to the content and style of your book."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "tqxVh9s3xYZ3",
        "outputId": "97cca95d-4ab3-44d8-a76c-5713aad387d8"
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"Based on the provided context from 'AI for Everyone? Critical Perspectives', \n",
        "please answer the following question with a critical and analytical approach:\n",
        "\n",
        "Question: How do the authors challenge the notion of technological determinism in AI, \n",
        "and what alternative frameworks do they propose for understanding AI's role in society?\n",
        "\n",
        "Please provide:\n",
        "1. Key arguments from the text regarding power structures and AI inequalities\n",
        "2. Specific examples or case studies mentioned in the context\n",
        "3. Any theoretical frameworks or critical perspectives discussed\n",
        "4. The implications for policy and governance\n",
        "\n",
        "Use direct references from the context where possible, and maintain an academic tone \n",
        "consistent with critical theory perspectives.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0mjkQJ_ZxYZ3"
      },
      "outputs": [],
      "source": [
        "import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice3: Tune parameters like temperature, and penalties to control how creative, focused, or varied the model's responses are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ylypRWRlxYZ4"
      },
      "outputs": [],
      "source": [
        "# Set up GPT client and parameters\n",
        "client = openai.OpenAI()\n",
        "model_params = {\n",
        "    'model': 'gpt-4o',\n",
        "    'temperature': 0.7,  # Increase creativity\n",
        "    'max_tokens': 4000,  # Allow for longer responses\n",
        "    'top_p': 0.9,        # Use nucleus sampling\n",
        "    'frequency_penalty': 0.5,  # Reduce repetition\n",
        "    'presence_penalty': 0.6    # Encourage new topics\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8e942xDxYZ4"
      },
      "source": [
        "<h1 style=\"color: #FF6347;\">Response</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4eXZO4pIxYZ4"
      },
      "outputs": [],
      "source": [
        "messages = [{'role': 'user', 'content': prompt}]\n",
        "completion = client.chat.completions.create(messages=messages, **model_params, timeout=120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "wLPAcchBxYZ5",
        "outputId": "976c7800-16ed-41fe-c4cf-58f60d3230d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The authors of \"AI for Everyone? Critical Perspectives\" challenge the notion of technological determinism by emphasizing that AI does not develop in a vacuum but is deeply embedded within existing social, political, and economic structures. They argue that viewing AI as an autonomous force that inevitably shapes society overlooks the significant influence of human agency, power dynamics, and socio-economic contexts on its development and deployment.\n",
            "\n",
            "1. **Key Arguments Regarding Power Structures and AI Inequalities:**\n",
            "   - The text highlights how AI systems often reinforce existing power hierarchies and inequalities rather than disrupt them. This is due to the fact that those who control AI technologies typically come from dominant social groups with specific interests and biases.\n",
            "   - The authors point out that AI can exacerbate disparities by embedding biases into algorithms, thereby perpetuating discrimination in areas such as hiring practices, law enforcement, and access to resources.\n",
            "\n",
            "2. **Specific Examples or Case Studies:**\n",
            "   - One case study mentioned involves predictive policing algorithms, which have been shown to disproportionately target minority communities due to biased historical crime data. This example illustrates how AI can perpetuate systemic racism.\n",
            "   - Another example discussed is the use of AI in recruitment processes where algorithms trained on historical hiring data may favor certain demographics over others, leading to a lack of diversity in workplaces.\n",
            "\n",
            "3. **Theoretical Frameworks or Critical Perspectives:**\n",
            "   - The authors draw from critical theory frameworks which focus on understanding the role of ideology and power in shaping technology. They advocate for a socio-technical approach that considers both technological capabilities and their interplay with societal factors.\n",
            "   - Feminist theory is also employed to critique the gendered dimensions of AI development and application, highlighting issues like gender bias in machine learning datasets.\n",
            "\n",
            "4. **Implications for Policy and Governance:**\n",
            "   - The text suggests that policymakers should adopt a more holistic approach towards AI governance by incorporating ethical considerations and promoting transparency in algorithmic decision-making processes.\n",
            "   - There is a call for inclusive policy-making processes that involve diverse stakeholdersparticularly marginalized communitiesto ensure equitable outcomes.\n",
            "   - Regulatory frameworks should be designed to hold companies accountable for discriminatory practices arising from their use of AI technologies.\n",
            "\n",
            "In summary, the authors propose moving beyond deterministic views of technology by recognizing the complex interactions between AI systems and societal structures. They argue for alternative frameworks that emphasize human agency, accountability, and inclusivity in order to better understandand governthe role of AI in society.\n"
          ]
        }
      ],
      "source": [
        "answer = completion.choices[0].message.content\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXVNXPwLxYaT"
      },
      "source": [
        "<img src=\"https://miro.medium.com/v2/resize:fit:824/1*GK56xmDIWtNQAD_jnBIt2g.png\" alt=\"NLP Gif\" style=\"width: 500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldybhlqKxYaT"
      },
      "source": [
        "<h2 style=\"color: #FF6347;\">Cosine Similarity</h2>\n",
        "\n",
        "**Cosine similarity** is a metric used to measure the alignment or similarity between two vectors, calculated as the cosine of the angle between them. It is the **most common metric used in RAG pipelines** for vector retrieval.. It provides a scale from -1 to 1:\n",
        "\n",
        "- **-1**: Vectors are completely opposite.\n",
        "- **0**: Vectors are orthogonal (uncorrelated or unrelated).\n",
        "- **1**: Vectors are identical.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c1I1TNhxYaT"
      },
      "source": [
        "<img src=\"https://storage.googleapis.com/lds-media/images/cosine-similarity-vectors.original.jpg\" alt=\"NLP Gif\" style=\"width: 700px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoEMdNgQxYaU"
      },
      "source": [
        "<h2 style=\"color: #FF6347;\">Keyword Highlighting</h2>\n",
        "\n",
        "Highlighting important keywords helps users quickly understand the relevance of the retrieved text to their query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting termcolor\n",
            "  Using cached termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Using cached termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
            "Installing collected packages: termcolor\n",
            "Successfully installed termcolor-3.2.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install termcolor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nCXL9Cz1xYaV"
      },
      "outputs": [],
      "source": [
        "from termcolor import colored"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwDyofY0xYaV"
      },
      "source": [
        "The `highlight_keywords` function is designed to highlight specific keywords within a given text. It replaces each keyword in the text with a highlighted version using the `colored` function from the `termcolor` library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9y3E0YWExYaV"
      },
      "outputs": [],
      "source": [
        "def highlight_keywords(text, keywords):\n",
        "    for keyword in keywords:\n",
        "        text = text.replace(keyword, colored(keyword, 'green', attrs=['bold']))\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice4: add your keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "i7SkWPpnxYaW",
        "outputId": "28e82563-edba-4b41-acad-ec27e5ba134f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Snippet 1:\n",
            "Watzlawick, P . 1964. An Anthology of Human Communication. Palo Alto, CA: \n",
            "Science and Behavior Books.\n",
            "Weizenbaum, J. 1976. Computer Power and Human Reason: From Judgment to \n",
            "Calculation. San Francisc\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "query_keywords = [\"AI\", \"power\", \"critical\", \"technology\", \"inequality\"] # add your keywords\n",
        "for i, doc in enumerate(retrieved_docs[:1]):\n",
        "    snippet = doc.page_content[:200]\n",
        "    highlighted = highlight_keywords(snippet, query_keywords)\n",
        "    print(f\"Snippet {i+1}:\\n{highlighted}\\n{'-'*80}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhV_Jf_LxYaX"
      },
      "source": [
        "1. `query_keywords` is a list of keywords to be highlighted.\n",
        "2. The loop iterates over the first document in retrieved_docs.\n",
        "3. For each document, a snippet of the first 200 characters is extracted.\n",
        "4. The highlight_keywords function is called to highlight the keywords in the snippet.\n",
        "5. The highlighted snippet is printed along with a separator line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBRKysAvxYaX"
      },
      "source": [
        "<h1 style=\"color: #FF6347;\">Bonus</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj25lCybxYaX"
      },
      "source": [
        "**Try loading one of your own PDF books and go through the steps again to explore how the pipeline works with your content**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "STEP 1: CONFIGURATION\n",
            "================================================================================\n",
            " OpenAI API key loaded successfully\n",
            "\n",
            "================================================================================\n",
            "STEP 2: LOADING PDF DOCUMENT\n",
            "================================================================================\n",
            " Successfully loaded PDF: /Users/Shyam/Desktop/Ironhack-Bootcamp/Week 7/D5/lab-intro-rag/steel_axle_thesis.pdf\n",
            " Total pages: 80\n",
            "\n",
            " Sample from first page (first 300 characters):\n",
            "--------------------------------------------------------------------------------\n",
            "Investigation on material properties\n",
            "of hardened steel axle and influence of\n",
            "die-casting process and data analysis\n",
            "Submitted in the partial fulfillment of the requirements for the\n",
            "award of degree Masters of Science In\n",
            "Metallic Materials Technology\n",
            "Submitted by\n",
            "Shyam sunder Chiliveri\n",
            "Matriculation Nu...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "STEP 3: SPLITTING DOCUMENT INTO CHUNKS\n",
            "================================================================================\n",
            " Document split into 397 chunks\n",
            "  Chunk size: 450 characters\n",
            "  Chunk overlap: 50 characters\n",
            "\n",
            " Sample chunk:\n",
            "--------------------------------------------------------------------------------\n",
            "Investigation on material properties\n",
            "of hardened steel axle and influence of\n",
            "die-casting process and data analysis\n",
            "Submitted in the partial fulfillment of the requirements for the\n",
            "award of degree Masters of Science In\n",
            "Metallic Materials Technology\n",
            "Submitted by\n",
            "Shyam sunder Chiliveri\n",
            "Matriculation Number: 66064\n",
            "Duration: 01.11.2023 to\n",
            "With the guidance of\n",
            "Supervisor: Thomas Rosemann\n",
            "Prof. Dr.-Ing. ...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "STEP 4: CREATING EMBEDDINGS\n",
            "================================================================================\n",
            " Embeddings model initialized: text-embedding-3-large\n",
            "\n",
            "================================================================================\n",
            "STEP 5: CREATING VECTOR DATABASE\n",
            "================================================================================\n",
            " ChromaDB created successfully\n",
            " Database stored at: ./my_custom_chroma_db\n",
            "\n",
            "================================================================================\n",
            "STEP 6: QUERYING YOUR DOCUMENT\n",
            "================================================================================\n",
            "Sample questions you can ask:\n",
            "  1. What is the vickers hardness?\n",
            "  2. What is the bending force?\n",
            "  3. Summarize the main arguments presented.\n",
            "\n",
            " Searching for: 'What is the vickers hardness?'\n",
            " Retrieved 3 relevant chunks\n",
            "\n",
            "================================================================================\n",
            "STEP 7: RETRIEVED RESULTS (with keyword highlighting)\n",
            "================================================================================\n",
            "\n",
            " Result 1:\n",
            "--------------------------------------------------------------------------------\n",
            "Chapter 5\n",
            "Analysis methods\n",
            "5.1 Vickers \u001b[1m\u001b[32mhardness\u001b[0m test\n",
            "The Vickers \u001b[1m\u001b[32mhardness\u001b[0m test is a method for measuring the \u001b[1m\u001b[32mhardness\u001b[0m of a material, typically\n",
            "metals. Its based on the principle of pressing a hardened steel or diamond indenter into\n",
            "the surface of the material being tested under a specific load. The \u001b[1m\u001b[32mhardness\u001b[0m value is then\n",
            "determined based on the size of the indentation left on the materials surface after the load\n",
            "is removed [19].\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            " Result 2:\n",
            "--------------------------------------------------------------------------------\n",
            "Chapter 5\n",
            "Analysis methods\n",
            "5.1 Vickers \u001b[1m\u001b[32mhardness\u001b[0m test\n",
            "The Vickers \u001b[1m\u001b[32mhardness\u001b[0m test is a method for measuring the \u001b[1m\u001b[32mhardness\u001b[0m of a material, typically\n",
            "metals. Its based on the principle of pressing a hardened steel or diamond indenter into\n",
            "the surface of the material being tested under a specific load. The \u001b[1m\u001b[32mhardness\u001b[0m value is then\n",
            "determined based on the size of the indentation left on the materials surface after the load\n",
            "is removed [19].\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            " Result 3:\n",
            "--------------------------------------------------------------------------------\n",
            "Chapter 5\n",
            "Analysis methods\n",
            "5.1 Vickers \u001b[1m\u001b[32mhardness\u001b[0m test\n",
            "The Vickers \u001b[1m\u001b[32mhardness\u001b[0m test is a method for measuring the \u001b[1m\u001b[32mhardness\u001b[0m of a material, typically\n",
            "metals. Its based on the principle of pressing a hardened steel or diamond indenter into\n",
            "the surface of the material being tested under a specific load. The \u001b[1m\u001b[32mhardness\u001b[0m value is then\n",
            "determined based on the size of the indentation left on the materials surface after the load\n",
            "is removed [19].\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "STEP 8: PREPARING CONTEXT FOR LLM\n",
            "================================================================================\n",
            " Context prepared (1351 characters)\n",
            "\n",
            " This context can now be sent to an LLM along with your question.\n",
            "\n",
            "================================================================================\n",
            "STEP 9: EXAMPLE LLM PROMPT\n",
            "================================================================================\n",
            "Example prompt structure:\n",
            "--------------------------------------------------------------------------------\n",
            "Based on the following context from the document, please answer this question:\n",
            "\n",
            "Question: What is the vickers hardness?\n",
            "\n",
            "Context:\n",
            "\n",
            "\n",
            "[Document 1]:\n",
            "Chapter 5\n",
            "Analysis methods\n",
            "5.1 Vickers hardness test\n",
            "The Vickers hardness test is a method for measuring the hardness of a material, typically\n",
            "metals. Its based on the principle of pressing a hardened steel or diamond indenter into\n",
            "the surface of the material being tested under a specific load. The hardness value is then\n",
            "determined based on the size of the indentation left on the materials surface after the load\n",
            "is removed [19].\n",
            "\n",
            "[Document 2]:\n",
            "Chapter 5\n",
            "Analysis methods\n",
            "5.1 Vickers hardness test\n",
            "The Vickers hardness test is a method for measuring the hardness of a material, typically\n",
            "metals. Its based on the principle of pressing a hardened steel or diamond indenter into\n",
            "the surface of the material being tested under a specific load. The hardness value is then\n",
            "determined based on the size of the indentation left on the materials surface after the load\n",
            "is removed [19].\n",
            "\n",
            "[Document 3]:\n",
            "Chapter 5\n",
            "Analysis methods\n",
            "5.1 Vickers hardness test\n",
            "The Vickers hardness test is a ... (truncated for display)\n",
            "\n",
            "Please provide a detailed answer based solely on the information in the context provided.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            " RAG PIPELINE COMPLETED SUCCESSFULLY!\n",
            "================================================================================\n",
            "\n",
            " Summary:\n",
            "   PDF loaded: 80 pages\n",
            "   Chunks created: 397\n",
            "   Embeddings model: text-embedding-3-large\n",
            "   Vector database: ChromaDB\n",
            "   Retrieved chunks: 3\n",
            "\n",
            " Next Steps:\n",
            "  1. Modify YOUR_PDF_PATH to point to your PDF\n",
            "  2. Customize the questions in sample_questions\n",
            "  3. Adjust keywords for highlighting\n",
            "  4. Connect to an LLM (OpenAI, Claude, etc.) to generate answers\n",
            "  5. Experiment with different chunk sizes and overlap values\n",
            "  6. Try different values of k (number of retrieved documents)\n",
            "\n",
            " Tips:\n",
            "   For technical documents, use smaller chunks (500-800 chars)\n",
            "   For narrative content, use larger chunks (1000-1500 chars)\n",
            "   Increase k if you want more context, but watch token limits\n",
            "   Use specific keywords from your domain for better highlighting\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Complete RAG Pipeline for Your Own PDF\n",
        "=======================================\n",
        "This script walks you through the entire RAG process with your own PDF document.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from dotenv import load_dotenv\n",
        "from termcolor import colored\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: CONFIGURATION\n",
        "# ============================================================================\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 1: CONFIGURATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# TODO: Replace with your PDF file path\n",
        "YOUR_PDF_PATH = \"/Users/Shyam/Desktop/Ironhack-Bootcamp/Week 7/D5/lab-intro-rag/steel_axle_thesis.pdf\"\n",
        "# Load environment variables (for OpenAI API key)\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if not api_key:\n",
        "    print(\"  WARNING: OPENAI_API_KEY not found in environment variables!\")\n",
        "    print(\"Please create a .env file with your API key or set it as an environment variable.\")\n",
        "else:\n",
        "    print(\" OpenAI API key loaded successfully\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: LOAD YOUR PDF DOCUMENT\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2: LOADING PDF DOCUMENT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "try:\n",
        "    loader = PyPDFLoader(YOUR_PDF_PATH)\n",
        "    pages = loader.load_and_split()\n",
        "    print(f\" Successfully loaded PDF: {YOUR_PDF_PATH}\")\n",
        "    print(f\" Total pages: {len(pages)}\")\n",
        "    print(f\"\\n Sample from first page (first 300 characters):\")\n",
        "    print(\"-\" * 80)\n",
        "    print(pages[0].page_content[:300] + \"...\")\n",
        "    print(\"-\" * 80)\n",
        "except FileNotFoundError:\n",
        "    print(f\" ERROR: File not found at '{YOUR_PDF_PATH}'\")\n",
        "    print(\"Please update YOUR_PDF_PATH with the correct path to your PDF.\")\n",
        "    exit(1)\n",
        "except Exception as e:\n",
        "    print(f\" ERROR loading PDF: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: SPLIT DOCUMENTS INTO CHUNKS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3: SPLITTING DOCUMENT INTO CHUNKS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Configure the text splitter\n",
        "CHUNK_SIZE = 450  # Maximum characters per chunk\n",
        "CHUNK_OVERLAP = 50  # Overlapping characters between chunks\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(pages)\n",
        "print(f\" Document split into {len(chunks)} chunks\")\n",
        "print(f\"  Chunk size: {CHUNK_SIZE} characters\")\n",
        "print(f\"  Chunk overlap: {CHUNK_OVERLAP} characters\")\n",
        "print(f\"\\n Sample chunk:\")\n",
        "print(\"-\" * 80)\n",
        "print(chunks[0].page_content[:400] + \"...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: CREATE EMBEDDINGS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 4: CREATING EMBEDDINGS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "print(\" Embeddings model initialized: text-embedding-3-large\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: CREATE VECTOR DATABASE (ChromaDB)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5: CREATING VECTOR DATABASE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "db = Chroma.from_documents(\n",
        "    chunks, \n",
        "    embeddings, \n",
        "    persist_directory=\"./my_custom_chroma_db\"\n",
        ")\n",
        "print(\" ChromaDB created successfully\")\n",
        "print(f\" Database stored at: ./my_custom_chroma_db\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: QUERY YOUR DOCUMENT (RETRIEVAL)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 6: QUERYING YOUR DOCUMENT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# TODO: Customize these questions based on your PDF content\n",
        "sample_questions = [\n",
        "    \"What is the vickers hardness?\",\n",
        "    \"What is the bending force?\",\n",
        "    \"Summarize the main arguments presented.\"\n",
        "]\n",
        "\n",
        "print(\"Sample questions you can ask:\")\n",
        "for i, q in enumerate(sample_questions, 1):\n",
        "    print(f\"  {i}. {q}\")\n",
        "\n",
        "# Use the first question as an example\n",
        "user_question = sample_questions[0]\n",
        "print(f\"\\n Searching for: '{user_question}'\")\n",
        "\n",
        "# Retrieve relevant documents\n",
        "k = 3  # Number of relevant chunks to retrieve\n",
        "retrieved_docs = db.similarity_search(user_question, k=k)\n",
        "\n",
        "print(f\" Retrieved {len(retrieved_docs)} relevant chunks\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: DISPLAY RESULTS WITH KEYWORD HIGHLIGHTING\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 7: RETRIEVED RESULTS (with keyword highlighting)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# TODO: Add keywords relevant to your query\n",
        "keywords = [\"main\", \"topic\", \"hardness\", \"bending force\", \"microstructure analysis\"]\n",
        "\n",
        "def highlight_keywords(text, keywords):\n",
        "    \"\"\"Highlight keywords in text using colored output\"\"\"\n",
        "    for keyword in keywords:\n",
        "        # Case-insensitive replacement\n",
        "        import re\n",
        "        pattern = re.compile(re.escape(keyword), re.IGNORECASE)\n",
        "        text = pattern.sub(colored(keyword, 'green', attrs=['bold']), text)\n",
        "    return text\n",
        "\n",
        "# Display top 3 results with highlighting\n",
        "for i, doc in enumerate(retrieved_docs[:3]):\n",
        "    print(f\"\\n Result {i+1}:\")\n",
        "    print(\"-\" * 80)\n",
        "    snippet = doc.page_content[:500]  # Show first 500 characters\n",
        "    highlighted = highlight_keywords(snippet, keywords)\n",
        "    print(highlighted)\n",
        "    if len(doc.page_content) > 500:\n",
        "        print(\"... (truncated)\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 8: PREPARE CONTEXT FOR LLM\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 8: PREPARING CONTEXT FOR LLM\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def format_documents_for_llm(docs):\n",
        "    \"\"\"Format retrieved documents into a context string for LLM\"\"\"\n",
        "    context = \"\\n\"\n",
        "    for i, doc in enumerate(docs, 1):\n",
        "        context += f\"\\n[Document {i}]:\\n\"\n",
        "        context += doc.page_content + \"\\n\"\n",
        "    return context\n",
        "\n",
        "context = format_documents_for_llm(retrieved_docs)\n",
        "print(f\" Context prepared ({len(context)} characters)\")\n",
        "print(f\"\\n This context can now be sent to an LLM along with your question.\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 9: EXAMPLE LLM PROMPT\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 9: EXAMPLE LLM PROMPT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "example_prompt = f\"\"\"Based on the following context from the document, please answer this question:\n",
        "\n",
        "Question: {user_question}\n",
        "\n",
        "Context:\n",
        "{context[:1000]}... (truncated for display)\n",
        "\n",
        "Please provide a detailed answer based solely on the information in the context provided.\"\"\"\n",
        "\n",
        "print(\"Example prompt structure:\")\n",
        "print(\"-\" * 80)\n",
        "print(example_prompt)\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY & NEXT STEPS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\" RAG PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n Summary:\")\n",
        "print(f\"   PDF loaded: {len(pages)} pages\")\n",
        "print(f\"   Chunks created: {len(chunks)}\")\n",
        "print(f\"   Embeddings model: text-embedding-3-large\")\n",
        "print(f\"   Vector database: ChromaDB\")\n",
        "print(f\"   Retrieved chunks: {k}\")\n",
        "\n",
        "print(\"\\n Next Steps:\")\n",
        "print(\"  1. Modify YOUR_PDF_PATH to point to your PDF\")\n",
        "print(\"  2. Customize the questions in sample_questions\")\n",
        "print(\"  3. Adjust keywords for highlighting\")\n",
        "print(\"  4. Connect to an LLM (OpenAI, Claude, etc.) to generate answers\")\n",
        "print(\"  5. Experiment with different chunk sizes and overlap values\")\n",
        "print(\"  6. Try different values of k (number of retrieved documents)\")\n",
        "\n",
        "print(\"\\n Tips:\")\n",
        "print(\"   For technical documents, use smaller chunks (500-800 chars)\")\n",
        "print(\"   For narrative content, use larger chunks (1000-1500 chars)\")\n",
        "print(\"   Increase k if you want more context, but watch token limits\")\n",
        "print(\"   Use specific keywords from your domain for better highlighting\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
